<p align="center">
<img src="images/banner.png"  height=400>
</p>


El machine learning es una rama de la inteligencia artificial que permite a las m치quinas aprender y mejorar a partir de datos y experiencias sin ser expl칤citamente programadas. Su importancia radica en su capacidad para analizar grandes vol칰menes de datos, identificar patrones y hacer predicciones precisas, lo que impulsa avances en 치reas como la medicina, finanzas, marketing, y tecnolog칤a, mejorando la eficiencia y toma de decisiones en diversas industrias.

# 칈ndice

* [Pre Procesamiento](#Pre-Procesamiento) 

* [Regresi칩n](#Regresi칩n) 
  * [Regresi칩n Lineal Simple](#Regresi칩n-Lineal-Simple) 
  * [Regresi칩n Lineal M칰ltiple](#Regresi칩n-Lineal-M칰ltiple) 
  * [Regresi칩n Lineal Polin칩mica](#Regresi칩n-Lineal-Polin칩mica) 
  * [Support Vector Machine Regresi칩n](#Support-Vector-Machine-Regresi칩n) 
  * [Decision Tree Regresi칩n](#Decision-Tree-Regresi칩n) 
  * [Random Forest Regresi칩n](#Random-Forest-Regresi칩n) 

* [Clasificaci칩n](#Clasificaci칩n)
  * [Regresi칩n Log칤stica](#Regresi칩n-Log칤stica) 
  * [K Nearest Neighbors](#K-Nearest-Neighbors) 
  * [Support Vector Machine Clasificaci칩n](#Support-Vector-Machine-Clasificaci칩n) 
  * [Naive Bayes](#Naive-Bayes) 
  * [Decision Tree Clasificaci칩n](#Decision-Tree-Clasificaci칩n) 
  * [Random Forest Clasificaci칩n](#Random-Forest-Clasificaci칩n)  

* [Agrupamiento](#Agrupamiento) 
  * [K Means](#K-Means) 
  * [Agrupamiento Jer치rquico](#Agrupamiento-Jer치rquico) 

* [Reglas de Asociaci칩n](#Reglas-de-Asociaci칩n) 
  * [Apriori](#Apriori) 
  * [Eclat](#Eclat) 

* [Aprendizaje por Refuerzo](#Aprendizaje-por-Refuerzo) 
  * [Upper Confidence Bound](#Upper-Confidence-Bound) 
  * [Muestreo Thompson](#Muestreo-Thompson) 

* [Procesamiento de Lenguaje Natural](#Procesamiento-de-Lenguaje-Natural) 

* [Redes Neuronales](#Redes-Neuronales) 
  * [Redes Neuronales Artificiales](#Redes-Neuronales-Artificiales) 
  * [Redes Neuronales Convolucionales](#Redes-Neuronales-Convolucionales) 

* [Reducci칩n de Dimensiones](#Redes-Neuronales) 
  * [An치lisis de Componentes Principales](#An치lisis-de-Componentes-Principales) 
  * [An치lisis Discriminate Lineal](#An치lisis-Discriminate-Lineal) 
  * [An치lisis de Componentes Principales Kernel](#An치lisis-de-Componentes-Principales-Kernel) 

* [Selecci칩n de Modelos](#Seleccion-de-Modelos) 
  * [K Fold Cross Validation](#K-Fold-Cross-Validation) 
  * [Grid Search](#Grid-Search) 

* [Gradient Boosting](#Gradient-Boosting) 

* [Autor](#Autor)


*NOTA:* Se us칩 rutas absolutas por lo que debes verificar el path que usar치s al correr los c칩digos.


# [Pre Procesamiento](https://github.com/jrguignan/Machine_Learning/tree/main/Pre_Procesamiento)

[Preprocesamietno-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Pre_Procesamiento/pre_procesamiento.ipynb) - [Preprocesamietno-R](https://github.com/jrguignan/Machine_Learning/blob/main/Pre_Procesamiento/pre_procesamiento.R)

El preprocesamiento de datos es una etapa crucial en el flujo de trabajo de machine learning que involucra la limpieza y transformaci칩n de datos brutos para prepararlos para el an치lisis y modelado. La importancia del preprocesamiento radica en su capacidad para mejorar la calidad y la utilidad de los datos, lo cual es fundamental para el rendimiento y la precisi칩n de los modelos de machine learning. Datos bien preprocesados pueden reducir el ruido, manejar la multicolinealidad y garantizar que los algoritmos de aprendizaje funcionen de manera m치s eficiente y efectiva, conduciendo a modelos m치s robustos y resultados m치s confiables.

En esta secci칩n se revisar치n varios de los factores a tener en cuenta.

- Datos Faltantes
- Tratamiento de variables
- Normalizaci칩n y escalado de Datos
- Divisi칩n de Datos

<br>[Volver al 칈ndice](#칈ndice)

# [Regresi칩n](https://github.com/jrguignan/Machine_Learning/tree/main/Regresion)

Es un m칠todo estad칤stico y de aprendizaje autom치tico utilizado para modelar la relaci칩n entre una variable dependiente y una o m치s variables independientes. En t칠rminos simples, la regresi칩n permite predecir el valor de una variable basada en los valores de otras variables, identificando y cuantificando las conexiones entre ellas. Este enfoque es fundamental en diversas aplicaciones

Para poder aplicar un modelo de regresi칩n lineal se debe tener en cuenta:
- Linealidad
- Homocedasticidad
- Normalidad Multivariable
- Independencia de los Errores
- Ausencia de Multicolinealidad


## Regresi칩n Lineal Simple

[Regresion Lineal Simple-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_lineal_simple.ipynb) - [Regresion Lineal Simple-R](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_lineal_simple.R)

El modelo de regresi칩n lineal simple es una t칠cnica estad칤stica utilizada para predecir el valor de una variable dependiente bas치ndose en el valor de una variable independiente. Este modelo asume una relaci칩n lineal entre las dos variables y se representa con la ecuaci칩n y=mx+b, donde y es la variable dependiente, x es la variable independiente, m es la pendiente de la l칤nea y b es la intersecci칩n en el eje y. La importancia de los modelos de regresi칩n lineal simple radica en su capacidad para identificar y cuantificar relaciones entre variables, permitiendo predicciones y an치lisis de tendencias en una variedad de campos como la econom칤a, la biolog칤a y la ingenier칤a. Adem치s, su simplicidad y facilidad de interpretaci칩n lo convierten en una herramienta fundamental y accesible para el an치lisis de datos y la toma de decisiones basada en evidencia.

## Regresi칩n Lineal M칰ltiple

[Regresion Lineal M칰ltiple-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_lineal_multiple.ipynb) - [Regresion Lineal M칰ltiple-R](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_lineal_multiple.R)

El modelo de regresi칩n lineal m칰ltiple es una extensi칩n de la regresi칩n lineal simple que permite predecir el valor de una variable dependiente bas치ndose en m칰ltiples variables independientes. Representado por la ecuaci칩n 洧녽=洧녪0+洧녪1洧논1+洧녪2洧논2+...+洧녪洧녵洧논洧녵 , donde 洧녽 es la variable dependiente y 洧논1,洧논2,...,洧논洧녵 son las variables independientes, este modelo captura la relaci칩n conjunta entre varias variables y su impacto en el resultado. La importancia de los modelos de regresi칩n lineal m칰ltiple radica en su capacidad para manejar situaciones m치s complejas y realistas donde m칰ltiples factores influyen en el resultado, proporcionando un an치lisis m치s robusto y preciso. Esta t칠cnica es crucial en campos como la econom칤a, el marketing, la medicina y las ciencias sociales, donde se requiere comprender y predecir resultados basados en m칰ltiples influencias simult치neamente

M칠todo de cosntruccion del modelo final:
- Exahustivo (all in)
- Eliminaci칩n hacia atr치s
- Selecci칩n hacia adelante
- Eliminaci칩n Bilateral
- Comparaci칩n de scores

Estos m칠todos de construccion permiten ir ajustando el modelo a las variables independiente que nos pueden producir un mejor resultado. 

<br>[Volver al 칈ndice](#칈ndice)

## Regresi칩n Lineal Polin칩mica

[Regresion Lineal Polin칩mica-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_lineal_polinomica.ipynb) - [Regresion Lineal Polin칩mica-R](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_lineal_polinomica.R)


El modelo de regresi칩n lineal polin칩mica es una variante de la regresi칩n lineal que permite capturar relaciones no lineales entre las variables independientes y la variable dependiente al incluir t칠rminos polin칩micos (cuadr치ticos, c칰bicos, etc.) de las variables independientes. Representado por la ecuaci칩n 
洧녽=洧녪0+洧녪1洧논+洧녪2洧논^2+...+洧녪洧녵洧논^洧녵, este modelo es importante porque proporciona una mayor flexibilidad para ajustar y predecir datos que no siguen una tendencia lineal, mejorando as칤 la precisi칩n y el ajuste del modelo. 

## Support Vector Machine Regresi칩n

[Support Vector Machine-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_SVM.ipynb) - [Support Vector Machine-R](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_SVM.R)


Los modelos de Support Vector Machine (SVM) en regresi칩n, conocidos como Support Vector Regression (SVR), son una t칠cnica que busca encontrar la funci칩n que devuelva el m칤nimo error posible, manteniendo un margen de tolerancia 洧랬. SVR utiliza un enfoque similar al de clasificaci칩n SVM, creando un tubo alrededor de la funci칩n objetivo donde las desviaciones menores a 洧랬 no se consideran como errores. La importancia de SVR radica en su capacidad para manejar grandes cantidades de datos y su eficacia en problemas de alta dimensionalidad. Adem치s, SVR es robusto ante outliers y es capaz de modelar relaciones complejas no lineales mediante el uso de kernels, lo que lo convierte en una herramienta poderosa para tareas de predicci칩n.

<br>[Volver al 칈ndice](#칈ndice)

## Decision Tree Regresi칩n

[Decision Tree-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_arboles_decision.ipynb) - [Decision Tree-R](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_arboles_decision.R)

Los modelos de Decision Tree en regresi칩n son m칠todos no param칠tricos que dividen los datos en segmentos bas치ndose en valores de caracter칤sticas, creando un 치rbol de decisiones donde cada hoja representa un valor de predicci칩n. Cada nodo en el 치rbol divide los datos en subconjuntos m치s homog칠neos en cuanto a la variable objetivo. La importancia de los Decision Trees en regresi칩n radica en su simplicidad, interpretabilidad y capacidad para capturar relaciones no lineales y complejas entre las caracter칤sticas y la variable objetivo. Son robustos ante datos faltantes y permiten la incorporaci칩n de caracter칤sticas categ칩ricas sin necesidad de codificaci칩n. Adem치s, al ser la base de m칠todos avanzados como Random Forest y Gradient Boosting, los Decision Trees son fundamentales para mejorar la precisi칩n y reducir el sobreajuste en los modelos predictivos.

## Random Forest Regresi칩n

[Random Forest-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_bosques_aleatorios.ipynb) - [Random Forest-R](https://github.com/jrguignan/Machine_Learning/blob/main/Regresion/regresion_bosques_aleatorios.R)

Un modelo de regresi칩n de Random Forest es una t칠cnica de aprendizaje autom치tico que utiliza m칰ltiples 치rboles de decisi칩n para realizar predicciones m치s precisas. Este m칠todo consiste en construir numerosos 치rboles de decisi칩n en el proceso de entrenamiento y obtener el promedio de sus predicciones para la regresi칩n. La importancia de los modelos de Random Forest radica en su capacidad para manejar grandes cantidades de datos y caracter칤sticas sin requerir demasiada preparaci칩n de datos. Adem치s, son robustos frente al sobreajuste, ya que la combinaci칩n de m칰ltiples 치rboles de decisi칩n reduce la varianza del modelo, proporcionando resultados m치s confiables y estables.

<br>[Volver al 칈ndice](#칈ndice)





# [Clasificaci칩n](https://github.com/jrguignan/Machine_Learning/tree/main/Clasificacion)

La clasificaci칩n en machine learning es una t칠cnica utilizada para asignar categor칤as a datos nuevos bas치ndose en patrones aprendidos de un conjunto de datos etiquetados. Se trata de un problema supervisado donde el algoritmo aprende a distinguir entre diferentes clases durante la fase de entrenamiento y luego aplica ese conocimiento para predecir la clase de nuevas observaciones. La clasificaci칩n es crucial en numerosas aplicaciones, como el reconocimiento de voz, la detecci칩n de spam, el diagn칩stico m칠dico y la segmentaci칩n de clientes.

## Regresi칩n Log칤stica

[Regresi칩n Log칤stica-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/regresion%20_logistica.ipynb) - [Regresi칩n Log칤stica-R](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/regresion%20_logistica.R)

La Regresi칩n Log칤stica es una t칠cnica de aprendizaje supervisado utilizadas principalmente para la clasificaci칩n binaria o multiclase en machine learning. La Regresi칩n Log칤stica es un algoritmo lineal que estima la probabilidad de que una instancia pertenezca a una clase particular. Utiliza la funci칩n log칤stica para modelar la relaci칩n entre las variables independientes y la variable dependiente.

La importancia de la Regresi칩n Log칤stica radica en su simplicidad, eficiencia computacional y capacidad para proporcionar probabilidades bien calibradas. Es 칰til en problemas de clasificaci칩n donde se necesita entender la probabilidad de pertenencia a una clase, como en la detecci칩n de spam en correos electr칩nicos, diagn칩sticos m칠dicos basados en s칤ntomas o predicci칩n de comportamientos basados en caracter칤sticas observadas.

## K Nearest Neighbors

[K-Nearest Neighbors-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/k_vecinos_cercanos.ipynb) - [K-Nearest Neighbors-R](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/k_vecinos_cercanos.R)


El modelo de K-Nearest Neighbors (K-NN) es un algoritmo de aprendizaje supervisado. Su funcionamiento se basa en asignar a una nueva instancia la clase m치s frecuente entre sus vecinos m치s cercanos en un espacio de caracter칤sticas, utilizando una m칠trica de distancia, como la distancia euclidiana.

La importancia del modelo K-NN radica en su simplicidad conceptual y su eficacia en situaciones donde los datos no siguen una distribuci칩n clara y pueden no ser linealmente separables. Es particularmente 칰til cuando no hay un modelo subyacente bien definido y se requiere una clasificaci칩n basada en la similitud de caracter칤sticas.

<br>[Volver al 칈ndice](#칈ndice)

## Support Vector Machine Clasificaci칩n

[Support Vector Machine-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/maquinas_soporte_vectorial.ipynb) - [Support Vector Machine-R](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/maquinas_soporte_vectorial.R)

Los modelos de Support Vector Machine (SVM) son poderosas herramientas de aprendizaje supervisado. En el contexto de clasificaci칩n, SVM busca encontrar el hiperplano que mejor separa las clases en un espacio de caracter칤sticas, maximizando la distancia entre los puntos m치s cercanos de las clases opuestas, conocidos como vectores de soporte. Esto se logra mediante la optimizaci칩n de un margen entre las clases, lo que proporciona robustez y generalizaci칩n a datos no etiquetados.

La importancia de SVM radica en su capacidad para manejar eficazmente datos de alta dimensionalidad y problemas de clasificaci칩n no lineales a trav칠s del uso de funciones kernel, que permiten transformar el espacio de caracter칤sticas en dimensiones superiores. Esto significa que SVM puede capturar relaciones complejas entre variables de entrada y salida, adapt치ndose a patrones m치s sutiles y no lineales que otros m칠todos m치s simples podr칤an perder. Adem치s, SVM es eficiente en el manejo de conjuntos de datos de tama침o moderado y puede ser regularizado para evitar el sobreajuste, garantizando as칤 una buena generalizaci칩n a nuevos datos. Sin embargo, su rendimiento puede verse afectado negativamente por el tiempo de entrenamiento en conjuntos de datos muy grandes y la necesidad de ajustar correctamente los par치metros del modelo y la elecci칩n adecuada del kernel para obtener resultados 칩ptimos.

## Naive Bayes

[Naive Bayes-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/naive_bayes.ipynb) - [Naive Bayes-R](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/naive_bayes.R)

Naive Bayes es un m칠todo popular en el campo del aprendizaje autom치tico supervisado, especialmente en problemas de clasificaci칩n. Se basa en el teorema de Bayes y asume que las caracter칤sticas son independientes entre s칤, lo cual es una simplificaci칩n poderosa y efectiva en muchos casos pr치cticos. Este modelo calcula la probabilidad condicional de que una instancia pertenezca a una clase espec칤fica dadas sus caracter칤sticas, utilizando la regla de Bayes.

La importancia de Naive Bayes radica en su simplicidad, eficiencia y capacidad para manejar grandes vol칰menes de datos. A menudo, funciona bien incluso con conjuntos de datos peque침os y es robusto frente a la presencia de caracter칤sticas irrelevantes. Adem치s, Naive Bayes tiende a generalizar bien en situaciones donde la suposici칩n de independencia no se cumple e incluso puede superar a m칠todos m치s complejos en t칠rminos de velocidad y requerimientos computacionales.Sin embargo, su principal limitaci칩n es la asunci칩n de independencia entre las caracter칤sticas, lo cual puede no ser realista en muchos contextos reales. Esto puede llevar a subestimar las relaciones entre las caracter칤sticas y, en consecuencia, reducir la precisi칩n del modelo. A pesar de estas limitaciones, Naive Bayes sigue siendo una opci칩n valiosa y ampliamente utilizada en aplicaciones de clasificaci칩n, especialmente cuando se trata de conjuntos de datos grandes y heterog칠neos.

<br>[Volver al 칈ndice](#칈ndice)

## Decision Tree Clasificaci칩n

[Decision Tree-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/arboles_decision.ipynb) - [Decision Tree-R](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/arboles_decision.R)


Los 치rboles de decisi칩n son modelos de aprendizaje autom치tico que estructuran decisiones basadas en m칰ltiples niveles de condiciones l칩gicas. En esencia, dividen el conjunto de datos en subconjuntos m치s peque침os mediante reglas simples sobre las caracter칤sticas, que eventualmente llevan a la predicci칩n de la variable objetivo. La importancia de los 치rboles de decisi칩n radica en su capacidad para manejar datos no lineales y complejos, as칤 como su capacidad para capturar interacciones entre caracter칤sticas que otros modelos lineales podr칤an pasar por alto. Adem치s, son f치ciles de interpretar y visualizar, lo que ayuda a los analistas a comprender c칩mo se toman las decisiones dentro del modelo. Sin embargo, los 치rboles de decisi칩n pueden ser propensos al sobreajuste si no se manejan correctamente, lo que puede llevar a una falta de generalizaci칩n en datos nuevos. A pesar de esto, su versatilidad y capacidad para capturar patrones complejos los hacen herramientas valiosas en una amplia gama de aplicaciones de clasificaci칩n y predicci칩n.

## Random Forest Clasificaci칩n

[Random Forest-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/bosques_aleatorios.ipynb) - [Random Forest-R](https://github.com/jrguignan/Machine_Learning/blob/main/Clasificacion/bosques_aleatorios.R)

Los modelos de Random Forest son una extensi칩n poderosa de los 치rboles de decisi칩n en el 치mbito del aprendizaje autom치tico. Consisten en un conjunto de 치rboles de decisi칩n individuales entrenados con diferentes subconjuntos de datos y caracter칤sticas, utilizando un proceso conocido como bootstrap sampling. La importancia de los Random Forests radica en su capacidad para reducir el sobreajuste en comparaci칩n con un solo 치rbol de decisi칩n, ya que combinan m칰ltiples 치rboles entrenados de manera independiente. Esto les permite capturar una mayor variedad de patrones en los datos y mejorar la precisi칩n de las predicciones. Adem치s, los Random Forests pueden manejar eficazmente conjuntos de datos grandes con muchas caracter칤sticas, manteniendo un buen rendimiento computacional. Su capacidad para proporcionar estimaciones de la importancia de las caracter칤sticas tambi칠n es una ventaja, ya que ayuda a los analistas a comprender qu칠 variables est치n contribuyendo m치s a las predicciones del modelo. En resumen, los Random Forests son ampliamente utilizados en problemas de clasificaci칩n debido a su robustez, capacidad de generalizaci칩n y facilidad para manejar conjuntos de datos complejos.

<br>[Volver al 칈ndice](#칈ndice)





# [Agrupamiento](https://github.com/jrguignan/Machine_Learning/tree/main/Agrupamiento)

El agrupamiento son t칠cnicas que consiste en dividir un conjunto de datos en grupos homog칠neos o cl칰steres, donde los elementos dentro de cada grupo son m치s similares entre s칤 que con los elementos de otros grupos. Es 칰til para descubrir patrones ocultos, segmentar datos para an치lisis espec칤ficos y simplificar conjuntos de datos complejos mediante la identificaci칩n de estructuras intr칤nsecas.

## K Means

[K Means-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Agrupamiento/k_medias.ipynb) - [K Means-R](https://github.com/jrguignan/Machine_Learning/blob/main/Agrupamiento/k_medias.R)

El modelo de K-Means es un algoritmo de agrupamiento ampliamente utilizado en el campo del aprendizaje autom치tico y la miner칤a de datos. Su objetivo principal es dividir un conjunto de datos en grupos o cl칰steres basados en similitudes entre las muestras. Utiliza la distancia euclidiana para asignar cada punto de datos al cl칰ster m치s cercano, donde el n칰mero de cl칰steres (K) se especifica previamente. La importancia del algoritmo de K-Means radica en su capacidad para identificar estructuras ocultas o patrones en conjuntos de datos no etiquetados de manera eficiente y escalable. Esto permite realizar an치lisis exploratorios de datos, segmentar clientes, detectar anomal칤as, entre otras aplicaciones, sin necesidad de etiquetas predefinidas. Adem치s, K-Means es relativamente f치cil de implementar y entender, lo que lo convierte en una herramienta poderosa para la exploraci칩n inicial de datos y la preparaci칩n para modelos de aprendizaje supervisado m치s avanzados.

- WCSS - M칠todo del Codo

El WCSS (Within-Cluster Sum of Squares) es una m칠trica utilizada para determinar el n칰mero 칩ptimo de cl칰steres en un algoritmo de agrupamiento, como K-Means. Representa la suma de las distancias euclidianas al cuadrado de cada punto de datos dentro de su cl칰ster respectivo.
  
## Agrupamiento Jer치rquico

[Agrupamiento Jer치rquico-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Agrupamiento/agrupamiento_jerarquico.ipynb) - [Agrupamiento Jer치rquico-R](https://github.com/jrguignan/Machine_Learning/blob/main/Agrupamiento/agrupamiento_jerarquico.R)


Los modelos de agrupamiento jer치rquico son t칠cnicas que agrupan datos en una estructura jer치rquica, formando cl칰steres anidados en lugar de asignar cada punto de datos a un 칰nico cl칰ster. Esto permite identificar relaciones complejas entre grupos dentro de los datos, facilitando la exploraci칩n y comprensi칩n de patrones emergentes. La importancia de estos modelos radica en su capacidad para manejar conjuntos de datos donde la estructura subyacente puede ser desconocida o variar en complejidad, ofreciendo flexibilidad en la interpretaci칩n de la estructura de agrupamiento.

<br>[Volver al 칈ndice](#칈ndice)





# [Reglas de Asociaci칩n](https://github.com/jrguignan/Machine_Learning/tree/main/Reglas_de_Asociacion)


Las reglas de asociaci칩n en machine learning son m칠todos utilizados para identificar relaciones significativas entre variables en grandes bases de datos. Estas reglas ayudan a descubrir patrones interesantes, como la probabilidad de que ciertos productos se compren juntos en un supermercado. Son especialmente 칰tiles en el an치lisis de mercados, recomendaciones de productos y miner칤a de datos, proporcionando insights valiosos sobre el comportamiento de los clientes y mejorando la toma de decisiones comerciales.

## Apriori

[Apriori-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Reglas_de_Asociacion/a_priori.ipynb) - [A Priori-R](https://github.com/jrguignan/Machine_Learning/blob/main/Reglas_de_Asociacion/a_priori.R)

El modelo Apriori es un algoritmo que se utiliza para identificar conjuntos de 칤tems frecuentemente asociados en grandes bases de datos. Este algoritmo es importante porque ayuda a descubrir relaciones ocultas en los datos, como la probabilidad de que ciertos productos se compren juntos. Al generar reglas de asociaci칩n basadas en la frecuencia de aparici칩n de los 칤tems, Apriori permite a los negocios mejorar sus estrategias de marketing, optimizar el dise침o de tiendas y realizar recomendaciones personalizadas a los clientes, lo que en 칰ltima instancia mejora la satisfacci칩n del cliente y aumenta las ventas.

## Eclat

[Eclat-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Reglas_de_Asociacion/eclat.ipynb) - [Eclat-R](https://github.com/jrguignan/Machine_Learning/blob/main/Reglas_de_Asociacion/eclat.R)

El modelo Eclat es un algoritmo eficiente para encontrar conjuntos frecuentes de 칤tems en bases de datos grandes. A diferencia de Apriori, Eclat utiliza un enfoque basado en la intersecci칩n de transacciones, lo que lo hace m치s r치pido en ciertos escenarios. La importancia de Eclat radica en su capacidad para descubrir relaciones significativas entre 칤tems de manera eficiente, lo que es esencial para tareas como la recomendaci칩n de productos, el an치lisis de cesta de la compra y la optimizaci칩n de inventarios. Al identificar patrones ocultos y asociaciones entre 칤tems, Eclat ayuda a las empresas a tomar decisiones informadas y a mejorar sus estrategias de marketing y ventas.

<br>[Volver al 칈ndice](#칈ndice)






# [Aprendizaje por Refuerzo](https://github.com/jrguignan/Machine_Learning/tree/main/Aprendizaje_por_Refuerzo)


El aprendizaje por refuerzo es una t칠cnica en la que un agente aprende a tomar decisiones secuenciales en un entorno, optimizando su comportamiento a trav칠s de recompensas y castigos. A medida que el agente interact칰a con el entorno, recibe feedback en forma de recompensas (positivas o negativas) basadas en sus acciones, lo que le permite ajustar su estrategia para maximizar la recompensa acumulada a largo plazo. Este enfoque es crucial en aplicaciones como juegos, rob칩tica y control aut칩nomo, donde las decisiones deben adaptarse din치micamente para lograr objetivos espec칤ficos.

## Upper Confidence Bound

[Upper Confidence Bound-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Aprendizaje_por_Refuerzo/limite_superior_de_confianza.ipynb) - [Upper Confidence Bound-R](https://github.com/jrguignan/Machine_Learning/blob/main/Aprendizaje_por_Refuerzo/limite_superior_de_confianza.R)


El modelo de Upper Confidence Bound (UCB) es un algoritmo de aprendizaje por refuerzo utilizado principalmente en problemas de multi-armed bandit, donde un agente debe elegir entre varias opciones con recompensas inciertas. UCB equilibra la exploraci칩n y la explotaci칩n al seleccionar opciones que tienen la mayor "confianza superior", es decir, aquellas que potencialmente ofrecen la mejor recompensa con mayor certeza. Este enfoque es importante porque permite al agente aprender de manera eficiente en entornos desconocidos, optimizando sus decisiones a lo largo del tiempo y minimizando las p칠rdidas potenciales, lo que es crucial en aplicaciones como la optimizaci칩n de anuncios y la gesti칩n de inventarios.

## Muestreo Thompson

[Muestreo Thompson-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Aprendizaje_por_Refuerzo/muestreo_thompson.ipynb) - [Muestreo Thompson-R](https://github.com/jrguignan/Machine_Learning/blob/main/Aprendizaje_por_Refuerzo/muestreo_thompson.R)


El modelo de Muestreo Thompson es un algoritmo de aprendizaje por refuerzo utilizado para abordar problemas de multi-armed bandit, donde un agente debe elegir entre varias acciones con recompensas inciertas. Este enfoque se basa en la probabilidad bayesiana para seleccionar acciones de acuerdo con su probabilidad de ser la mejor opci칩n, lo que permite un equilibrio din치mico entre la exploraci칩n de nuevas acciones y la explotaci칩n de acciones conocidas con altas recompensas. La importancia del Muestreo Thompson radica en su capacidad para adaptarse a entornos cambiantes y optimizar las decisiones del agente de manera eficiente, lo que es crucial en aplicaciones como la publicidad en l칤nea, la recomendaci칩n de contenidos y la gesti칩n de recursos.

<br>[Volver al 칈ndice](#칈ndice)




# [Procesamiento de Lenguaje Natural](https://github.com/jrguignan/Machine_Learning/tree/main/Procesamiento_de_Lenguaje_Natural)


El Procesamiento de Lenguaje Natural (NLP, por sus siglas en ingl칠s) es una rama de la inteligencia artificial que se enfoca en la interacci칩n entre las computadoras y el lenguaje humano, permitiendo a las m치quinas entender, interpretar y generar lenguaje de manera que sea valiosa. Su importancia radica en su capacidad para automatizar y mejorar tareas que implican el lenguaje, como la traducci칩n autom치tica, la generaci칩n de res칰menes, la b칰squeda de informaci칩n y el an치lisis de sentimientos, facilitando as칤 la comunicaci칩n y el acceso a la informaci칩n a escala global.

[LNP-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Procesamiento_de_Lenguaje_Natural/pln.ipynb) - [LNP-R](https://github.com/jrguignan/Machine_Learning/blob/main/Procesamiento_de_Lenguaje_Natural/pln.R)



<br>[Volver al 칈ndice](#칈ndice)




# [Redes Neuronales](https://github.com/jrguignan/Machine_Learning/tree/main/Redes_Neuronales)



## Redes Neuronales Artificiales

[RNA-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Redes_Neuronales/redes_neuronales_artificiales.ipynb) - [RNA-R](https://github.com/jrguignan/Machine_Learning/blob/main/Redes_Neuronales/redes_neuronales_artificiales.R)

Las redes neuronales son modelos computacionales inspirados en el funcionamiento del cerebro humano, dise침ados para reconocer patrones y aprender de grandes vol칰menes de datos. Su importancia radica en su capacidad para resolver problemas complejos en 치reas como reconocimiento de im치genes, procesamiento de lenguaje natural, predicci칩n de series temporales y m치s, lo que impulsa avances significativos en inteligencia artificial y machine learning.

## Redes Neuronales Convolucionales

[RNC-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Redes_Neuronales/redes_neuronales_convolucionales.ipynb)

Las redes neuronales convolucionales (CNNs) son un tipo de red neuronal especialmente dise침ada para procesar datos con una estructura de cuadr칤cula, como im치genes. Utilizan capas de convoluci칩n y agrupamiento para detectar caracter칤sticas locales y patrones espaciales en los datos, lo que las hace extremadamente efectivas para tareas de visi칩n por computadora como el reconocimiento de objetos y la clasificaci칩n de im치genes. Su importancia radica en su capacidad para lograr altos niveles de precisi칩n en problemas complejos de procesamiento de im치genes, impulsando avances en campos como la inteligencia artificial y el aprendizaje profundo.


Al ser el dataset pesado, no se decidi칩n por un medio alterno y  se puede descargar de este link:
[dataset-RNC](https://drive.google.com/file/d/1Kay-Ig6g2EyDBKPK0oK4UmGj8fCcifpb/view?usp=sharing)

<br>[Volver al 칈ndice](#칈ndice)




# [Reducci칩n de Dimensiones](https://github.com/jrguignan/Machine_Learning/tree/main/Reduccion_de_Dimension)


La reducci칩n de dimensiones es una t칠cnica en machine learning y an치lisis de datos que busca disminuir el n칰mero de variables en un conjunto de datos mientras se conserva la mayor cantidad de informaci칩n posible. Su importancia radica en la capacidad de simplificar modelos, mejorar la visualizaci칩n de datos y aumentar la eficiencia computacional, lo cual facilita el procesamiento y an치lisis de grandes vol칰menes de datos.

## An치lisis de Componentes Principales

[An치lisis de Componentes Principales-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Reduccion_de_Dimension/analisis_componentes_principales.ipynb) - [An치lisis de Componentes Principales-R](https://github.com/jrguignan/Machine_Learning/blob/main/Reduccion_de_Dimension/analisis_componentes_principales.R)

El An치lisis de Componentes Principales (PCA) es una t칠cnica que transforma un conjunto de variables posiblemente correlacionadas en un conjunto de variables no correlacionadas llamadas componentes principales. Estos componentes retienen la mayor parte de la variabilidad presente en los datos originales. La importancia del PCA radica en su capacidad para simplificar conjuntos de datos complejos, mejorar la eficiencia computacional y facilitar la visualizaci칩n y el an치lisis, permitiendo la identificaci칩n de patrones y relaciones subyacentes en los datos.

## An치lisis Discriminate Lineal

[An치lisis Discriminate Lineal-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Reduccion_de_Dimension/analisis_discriminante_lineal.ipynb) - [An치lisis Discriminate Lineal-R](https://github.com/jrguignan/Machine_Learning/blob/main/Reduccion_de_Dimension/analisis_discriminante_lineal.R)

El An치lisis Discriminante Lineal (LDA) es una t칠cnica de reducci칩n de dimensionalidad y clasificaci칩n que busca encontrar una combinaci칩n lineal de caracter칤sticas que mejor separen dos o m치s clases de objetos o eventos. La importancia del LDA radica en su capacidad para maximizar la separabilidad entre categor칤as, mejorando la precisi칩n de los modelos de clasificaci칩n y facilitando la interpretaci칩n y visualizaci칩n de datos en problemas donde la diferenciaci칩n de clases es crucial.

## An치lisis de Componentes Principales Kernel

[An치lisis de Componentes Principales - Kernel-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Reduccion_de_Dimension/analisis_componentes_principales-kernel.ipynb) - [An치lisis de Componentes Principales - Kernel-R](https://github.com/jrguignan/Machine_Learning/blob/main/Reduccion_de_Dimension/analisis_componentes_principales-kernel.R)


El An치lisis de Componentes Principales con Kernel (KPCA) es una extensi칩n no lineal del An치lisis de Componentes Principales (PCA), que utiliza funciones kernel para proyectar datos en un espacio de mayor dimensi칩n, permitiendo capturar relaciones no lineales entre las variables. La importancia del KPCA radica en su capacidad para mejorar la reducci칩n de dimensionalidad y la extracci칩n de caracter칤sticas en conjuntos de datos complejos, lo que resulta en una mejor representaci칩n y clasificaci칩n de los datos en aplicaciones de machine learning.

<br>[Volver al 칈ndice](#칈ndice)




# [Selecci칩n de Modelos](https://github.com/jrguignan/Machine_Learning/tree/main/Seleccion_de_Modelos)

La selecci칩n de modelos se refiere al proceso de elegir el algoritmo y la configuraci칩n 칩ptima que mejor se ajuste a los datos y al problema espec칤fico que se est치 abordando. Esto implica evaluar y comparar diferentes modelos en funci칩n de m칠tricas de rendimiento como precisi칩n, recall, y F1-score, entre otras. Es crucial porque permite identificar el modelo m치s adecuado para obtener predicciones precisas y confiables, optimizando as칤 el rendimiento del sistema y mejorando la toma de decisiones basadas en datos.

## K Fold Cross Validation

[K Fold Cross Validation-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Seleccion_de_Modelos/kpartes_validacion_cruzada.ipynb) - [K Fold Cross Validation-R](https://github.com/jrguignan/Machine_Learning/blob/main/Seleccion_de_Modelos/kpartes_validacion_cruzada.R)

El K-Fold Cross Validation es una t칠cnica de validaci칩n utilizada  para evaluar la capacidad de generalizaci칩n de un modelo. Consiste en dividir el conjunto de datos en K subconjuntos (folds) de igual tama침o. El modelo se entrena K veces, cada vez utilizando K-1 folds para entrenar y el fold restante para validar. La importancia de esta t칠cnica radica en su capacidad para proporcionar una estimaci칩n m치s robusta y fiable del rendimiento del modelo, mitigando problemas de sobreajuste y proporcionando una evaluaci칩n m치s completa del modelo con diferentes particiones de los datos.

## Grid Search

[Grid Search-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Seleccion_de_Modelos/busqueda_rejilla.ipynb) - [Grid Search-R](https://github.com/jrguignan/Machine_Learning/blob/main/Seleccion_de_Modelos/busqueda_rejilla.R)

El Grid Search es una t칠cnica de optimizaci칩n de hiperpar치metros que implica probar exhaustivamente todas las combinaciones posibles de un conjunto predefinido de par치metros para encontrar la mejor configuraci칩n para un modelo. Se realiza una b칰squeda en una "cuadr칤cula" de par치metros, evaluando el rendimiento del modelo para cada combinaci칩n mediante validaci칩n cruzada. Su importancia radica en su capacidad para identificar la mejor configuraci칩n de hiperpar치metros, mejorando as칤 el rendimiento y la precisi칩n del modelo en nuevas predicciones.

<br>[Volver al 칈ndice](#칈ndice)




# [Gradient Boosting](https://github.com/jrguignan/Machine_Learning/tree/main/Gradient_Boosting)

[Gradient Boosting-Python](https://github.com/jrguignan/Machine_Learning/blob/main/Gradient_Boosting/gradient_boosting.ipynb) - [Gradient Boosting-R](https://github.com/jrguignan/Machine_Learning/blob/main/Gradient_Boosting/gradient_boosting.R)

El Gradient Boosting es una t칠cnica que crea un modelo predictivo a partir de una serie de modelos m치s simples, generalmente 치rboles de decisi칩n. Cada modelo intenta corregir los errores del modelo anterior, mejorando progresivamente la precisi칩n del conjunto. Su importancia radica en su capacidad para manejar datos complejos y proporcionar predicciones altamente precisas, siendo particularmente 칰til en competiciones y aplicaciones de clasificaci칩n y regresi칩n.

<br>[Volver al 칈ndice](#칈ndice)



# Autor

- Jos칠 R. Guignan
- Mail: joserguignan@gmail.com
- Linkedin: [https://www.linkedin.com/in/jrguignan](https://www.linkedin.com/in/jrguignan)
